{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pytorch 初学"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Tensors 与numpy 中的ndarrays很像，pytorch可以支持GPU操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 创建空的tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5,3) # 5行3列的空的tensors\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 创建随机的一个随机数矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5109, 0.1927, 0.5499],\n",
      "        [0.8677, 0.8713, 0.9610],\n",
      "        [0.9356, 0.0391, 0.3159],\n",
      "        [0.0266, 0.7895, 0.6610],\n",
      "        [0.7188, 0.1331, 0.2180]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 创建0元素的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 直接从已经数据创建tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "x= torch.tensor([5,5,3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 创建新的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5,3,dtype=torch.double)\n",
    "print(x)\n",
    "# 根据现有的张量创建张量。 这些方法将重用输入张量的属性，例如， dtype，除非设置新的值进行覆盖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8914,  1.5704, -0.1844],\n",
      "        [ 0.7747, -0.6860, -0.5596],\n",
      "        [ 0.1804, -0.2909, -1.3262],\n",
      "        [-1.3021, -0.4132, -2.7060],\n",
      "        [ 0.8989, -0.7269,  1.3862]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn_like(x,dtype=torch.float)\n",
    "print(x)\n",
    "# 更新了x的dtype,保留了原始的x的size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 加法操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6333,  2.1744,  0.4975],\n",
      "        [ 1.5430, -0.5863, -0.1416],\n",
      "        [ 0.6954,  0.6694, -0.4113],\n",
      "        [-0.9279, -0.1156, -1.8519],\n",
      "        [ 1.5791,  0.1524,  2.1037]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5,3)\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6333,  2.1744,  0.4975],\n",
      "        [ 1.5430, -0.5863, -0.1416],\n",
      "        [ 0.6954,  0.6694, -0.4113],\n",
      "        [-0.9279, -0.1156, -1.8519],\n",
      "        [ 1.5791,  0.1524,  2.1037]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6333,  2.1744,  0.4975],\n",
      "        [ 1.5430, -0.5863, -0.1416],\n",
      "        [ 0.6954,  0.6694, -0.4113],\n",
      "        [-0.9279, -0.1156, -1.8519],\n",
      "        [ 1.5791,  0.1524,  2.1037]])\n"
     ]
    }
   ],
   "source": [
    "result = torch.empty(5,3)\n",
    "torch.add(x,y,out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6333,  2.1744,  0.4975],\n",
      "        [ 1.5430, -0.5863, -0.1416],\n",
      "        [ 0.6954,  0.6694, -0.4113],\n",
      "        [-0.9279, -0.1156, -1.8519],\n",
      "        [ 1.5791,  0.1524,  2.1037]])\n"
     ]
    }
   ],
   "source": [
    "# 加法操作，inplace 操作,替换y值，类似于y+=x\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 转化形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4,4)\n",
    "y= x.view(16)\n",
    "z=x.view(-1,8)  # the size -1 是根据其他维度进行计算出来的\n",
    "print(x.size(),y.size(),z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.numel(x)  # return the number of the input tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tensor 与numpy 的转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3])\n",
    "t = torch.as_tensor(a)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()  # 类似于a和b，a 变了，b也跟着变，类似于numpy 中的view的操作\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4487109080, 4807132064)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(a),id(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据在GPU上的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下代码只有在PyTorch GPU版本上才会执行，配的mac没有GPU，所以没有显示结果\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自动梯度求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习中通常需要对函数求梯度(gradient),pytorch提供的autograd包能够根据输入和前向传播过程自动构建计算图，并执行方向传播过程，后续将主要介绍autograd包实现自动求梯度的有关操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 自动求导的概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上节介绍的Tensor是这个包的核心类，如果将其属性 .required_grad 设置为True，将开始追踪(track)在其上的所有操作(可以利用链式法则进行梯度传播了)。计算完成后，可以调用.backward() 来完成所有的梯度计算。此Tensor的梯度将累积到.grad属性中。\n",
    "需要注意的是，如果调用y.backward()时，如果y是标量，则不需要为backward() 传入任何参数。其余情况，需要传入一个与y相同shape的Tensor。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果不想被继续追踪，可以调用.detach()将其追踪记录中分离出来，这样就可以防止将来的计算被追踪，这样梯度就传不过去了。此外还可以用with torch.no_grad() 将不想被追踪的操作代码块包裹起来，这样的方法在评估模型的时候常用，因为在评估模型时，不需要计算已经训练出的参数的的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function 类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function是另外一个很重要的类，Tensor和Function相互结合就可以构建一个记录有整个计算过程的有向无环图(DAG)？？？？ \n",
    "每个Tensor都有一个.grad_fn属性，该属性即创建Tensor的Function,就是说该Tensor是不是通过某些运算得到的，如果是，grad_fn返还一个与这些运算相关的对象，否则是None。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensor实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 创建一个Tensor并设置requires_grad=True\n",
    "x= torch.ones(2,2,requires_grad=True)\n",
    "print(x)\n",
    "print(x.grad_fn)# 返回结果为None,x是直接创建的，则说明该Tensor不是通过运算得到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x11eb55cc0>\n",
      "True False\n"
     ]
    }
   ],
   "source": [
    "y =x+2\n",
    "print(y)\n",
    "print(y.grad_fn)  ## AddBackward0，y是通过一个假发操作创建的\n",
    "\n",
    "\n",
    "'''\n",
    "想x这样直接通过创建的称为叶子节点，叶子节点对应grad_fn 是None\n",
    "\n",
    "\n",
    "'''\n",
    "print(x.is_leaf,y.is_leaf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 复杂运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "z = y*y*3\n",
    "out = z.mean()\n",
    "print(z,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x11eb65780>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "可以通过.requires_grad_()来用in_place的方式改变requires_grad的属性\n",
    "\"\"\"\n",
    "a= torch.randn(2,2)  # 缺失的情况下默认requires_grad=False\n",
    "a =(a*3)/(a-1)\n",
    "print(a.requires_grad)   # False\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)    # True\n",
    "b = (a*a).sum()\n",
    "print(b.grad_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "因为out是一个标量，所以调用backward()时不需要指定求导变量：\n",
    "\n",
    "'''\n",
    "out.backward()    # 等价于out.backward(torch.tensor(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看下out关于x的梯度\n",
    "$$\n",
    "\\frac { d ( o u t ) } { d x }\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "o = \\frac { 1 } { 4 } \\sum _ { i = 1 } ^ { 4 } z _ { i } = \\frac { 1 } { 4 } \\sum _ { i = 1 } ^ { 4 } 3 \\left( x _ { i } + 2 \\right) ^ { 2 }\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left. \\frac { \\partial o } { \\partial x _ { i } } \\right| _ { x _ { i } = 1 } = \\frac { 9 } { 2 } = 4.5\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 数学上的意义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数学上，如果有一个函数值和自变量都为向量的函数 y=f(x) \n",
    "y=f(x), 那么 y关于x 的梯度就是一个雅可比矩阵（Jacobian matrix）:\n",
    "$$\n",
    "J = \\left( \\begin{array} { c c c } { \\frac { \\partial y 1 } { \\partial x _ { 1 } } } & { \\cdots } & { \\frac { \\partial y _ { 1 } } { \\partial x _ { n } } } \\\\ { \\vdots } & { \\ddots } & { \\vdots } \\\\ { \\frac { \\partial y _ { m } } { \\partial x _ { 1 } } } & { \\cdots } & { \\frac { \\partial y _ { m } } { \\partial x _ { n } } } \\end{array} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而torch.autograd这个包就是用来计算一些雅克比矩阵的乘积的，例如如果v是一个标量函数的L = g(y)的梯度：\n",
    "\n",
    "$$\n",
    "v = \\left( \\begin{array} { c c c } { \\frac { \\partial l } { \\partial y 1 } } & { \\cdots } & { \\frac { \\partial l } { \\partial y m } } \\end{array} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "那么根据链式法则，可以得到：L关于x的雅克比矩阵就是\n",
    "\n",
    "$$\n",
    "v J = \\left( \\begin{array} { c c c } { \\frac { \\partial l } { \\partial y _ { 1 } } } & { \\cdots } & { \\frac { \\partial l } { \\partial y _ { m } } } \\end{array} \\right) \\left( \\begin{array} { c c c } { \\frac { \\partial y _ { 1 } } { \\partial x _ { 1 } } } & { \\cdots } & { \\frac { \\partial y _ { 1 } } { \\partial x _ { n } } } \\\\ { \\vdots } & { \\ddots } & { \\vdots } \\\\ { \\frac { \\partial y _ { m } } { \\partial x _ { 1 } } } & { \\cdots } & { \\frac { \\partial y _ { m } } { \\partial x _ { n } } } \\end{array} \\right) = \\left( \\begin{array} { c c c } { \\frac { \\partial l } { \\partial x _ { 1 } } } & { \\cdots } & { \\frac { \\partial l } { \\partial x _ { n } } } \\end{array} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "注意：grad在反向传播过程中是累加的(accumulated)，这意味着运行反向传播，梯度都会累加到前一次的梯度，所以一般在反正传播之前需要把梯度清零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.5000, 5.5000],\n",
      "        [5.5000, 5.5000]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 再来反向传播一次，注意grad是累加的\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "\n",
    "out3 = x.sum()\n",
    "x.grad.data.zero_()   # 梯度清零，将梯度的数据变成0\n",
    "out3.backward()\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在需要解释一个问题：\n",
    "为什么在y.backward()时，如果y是标量，责不需要为backward()传入任何参数；否则需要传入一个与y同形的Tensor?\n",
    "1. 首先为了避免向量(甚至更高维张量)对张量求导，而转换成标量对张量求导；\n",
    "2. 不允许张量对张量求导，只允许标量与张量求导，求导的结果是和自变量同形的张量。这个地方说的就是不能让函数对函数求导(估计是一个意思吧）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0,2.0,3.0,4.0],requires_grad=True)\n",
    "y = 2*x\n",
    "z= y.view(2,2)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在的y不是一个标量，所以在调用backward()时需要传入一个和y同形的权重向量进行加权就和得到一个标量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000, 2.0000, 0.0200, 0.0200])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([[1.0,1.0],[0.01,0.01]],dtype=torch.float)\n",
    "z.backward(v)   # 此时v就是与y同形的权重向量\n",
    "print(x.grad)   # x.grad是和x同形的张量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 中断梯度追踪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(1., grad_fn=<PowBackward0>) True\n",
      "tensor(1.) False\n",
      "tensor(2., grad_fn=<AddBackward0>) True\n",
      "tensor(1.) True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0,requires_grad=True)\n",
    "y1 = x**2\n",
    "with torch.no_grad():\n",
    "    y2 = x**3\n",
    "y3 = y1+y2\n",
    "print(x.requires_grad)\n",
    "print(y1,y1.requires_grad)   # 平方\n",
    "print(y2,y2.requires_grad)   # 标量，在with torch.no_grad未被追踪\n",
    "print(y3,y3.requires_grad)    # 求和\n",
    "print(y2,y2.is_leaf)  #  标量没有计算公式，y2也称为称为叶子节点，叶子节点对应grad_fn 是None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y3 = y1+y2 =x**2+ y**3,当x=1 时，dy3/dx应该是5，但是y2的定义被torch.no_grad()包裹的，\n",
    "所以与y2相关的梯度是不会被回传的，只有与y1有关的梯度才会回传，即x**2对x的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 修改tensor的数字，不被autograd记录(即不会影响方向传播)，可以对tensor.data 进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1,requires_grad=True)\n",
    "print(x.data) # 也是一个tensor\n",
    "print(x.data.requires_grad)  # 但是已经是独立于计算图之外\n",
    "y = 2*x\n",
    "\n",
    "x.data *=100 # 只改变了data属性值，不会记录在计算图，因此不会影响梯度传播\n",
    "y.backward()\n",
    "print(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
