{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### softmax回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前几节介绍的是线性回归模型适用于输出连续值的情况，在另外一类情况下，模型输出的是一个图像的类别这样的离散值。对于离散值预测的问题，我们可以采用诸如softmax回归在内的分类模型。\n",
    "和线性回归不同，softmax回归的输出单元从一个变成了多个，且引入了运算使输出值更适合离散值的预测和训练。以softmax回归模型为例，介绍神经网络中的分类模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们先考虑一个简单的图像分类维内托，其输入图像的高和宽均为2像素，且色彩是灰度。这样的像素值都可以用一个标量表示。我们将图像中的4像素分别记为$x_{1},x_{2},x_{3},x_{4}$。假设训练数据中图像的真实标签为狗，猫或者鸡(假设可以用4个像素表示出这三种动物),这些标签分别对应的离散值$y_{1},y_{2},y_{3}$。我们通常使用离散值的数值来表示类别，例如$y_{1}=1,y_{2}=2,y_{3}=3$。这样一张图片的标签为1，2和3这三个数值中的一个。但是这种连续值到离散值的转化通常会影响到分类的质量。因此我们使用更加适合离散值输出的模型来解决分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### softmax 回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax回归和线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于，softmax回归的输出值的个数等于标签里的类别数。因此一共有4个特征和3个类别的动物类别，所以权重包含12个标量带下标的$w$，偏差包含3个标量带下标的$b$，且对于每个输入计算$o_{1},o_{2},o_{3}$这三个输出\n",
    "\n",
    "$o_{1} = x_{1}w_{11} + x_{2}w_{21}+x_{3}w_{31}+x_{4}w_{41}+b_{1}$,\n",
    "$o_{2} = x_{1}w_{12} + x_{2}w_{22}+x_{3}w_{32}+x_{4}w_{42}+b_{2}$,\n",
    "$o_{3} = x_{1}w_{13} + x_{2}w_{23}+x_{3}w_{33}+x_{4}w_{43}+b_{2}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图用神经网络描绘了上面的计算。softmax回归同线性回归一样，也是一个单层神经网络。由于每个输出$o_{1},o_{2},o_{3}$的计算都有以来与所有的输入$x_{1},x_{2},x_{3},x_{4}$，softmax回归的输出层也是一个全连接层。\n",
    "![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8idoibq56j309c039748.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既然分类问题需要输出离散的预测结果，一个简单的方法就是把计算出来的$o_{1},o_{2},o_{3}$作为预测类别的是i的置信度，并将值最大的输出所对应的类作为预测输出，即\n",
    "$\\underset{i}{\\arg\\max}o_{i}$,如果$o_{1},o_{2},o_{3}$分别为0.1，10，0.1，由于$o_{2}$最大，那么预测的类别为2，其代表猫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 输出层的值的范围不确定，很难直观的判断这些值的意义。\n",
    "- 真实标签是离散值，离散值与不确定范围的输出值之间的误差难以衡量，\n",
    "而通过softmax运算解决了以上两个问题，它通过下式将输出值转换成值为正，且和为1的概率分布：\n",
    "$\\hat{y}_{1},\\hat{y}_{2},\\hat{y}_{3}$ = $softmax(o_{1},o_{2},o_{3})$\n",
    "其中 \n",
    "\n",
    "$$\n",
    "\\hat { y } _ { 1 } = \\frac { \\exp ( o_{1} ) } { \\sum _ { i = 1 } ^ { 3 } \\exp ( o_{i }) } , \\quad \\hat { y } _ { 2 } = \\frac { \\exp \\left( o _ { 2 } \\right) } { \\sum _ { i = 1 } ^ { 2 } \\exp \\left( o _ { i } \\right) } , \\quad \\hat { y } _ { 3 } = \\frac { \\exp \\left( o _ { 3 } \\right) } { \\sum _ { i = 1 } ^ { 3 } \\exp \\left( o _ { i } \\right) }\n",
    "$$\n",
    "\n",
    "容易看出$\\hat{y}_{1}+\\hat{y}_{2}+\\hat{y}_{3}=1$,且0<=$\\hat{y}_{1},\\hat{y}_{2},\\hat{y}_{3}$<=1。$\\hat{y}_{1},\\hat{y}_{2},\\hat{y}_{3}$是一个合理的概率分布。这时候，如果$\\hat{y}_{2}$=0.8,不管其他两个值是多少，我们都知道图像类别为猫的概率为80%，此外\n",
    "$\\underset{i}{\\arg\\max}o_{i}$  = $\\underset{i}{\\arg\\max}\\hat{y}_{i}$，softmax运算不会改变预测类别的输出结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 单样本分类的矢量计算表达式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了提高计算效率，我们可以将单样本分类通过矢量计算来表达。在上面的图像分类问题中，假设softmax回归的权重和偏差参数分别为\n",
    "$$\n",
    "\\boldsymbol { W } = \\left[ \\begin{array} { l l l } { w _ { 11 } } & { w _ { 12 } } & { w _ { 13 } } \\\\ { w _ { 21 } } & { w _ { 22 } } & { w _ { 23 } } \\\\ { w _ { 31 } } & { w _ { 32 } } & { w _ { 33 } } \\\\ { w _ { 41 } } & { w _ { 42 } } & { w _ { 43 } } \\end{array} \\right] , \\quad \\boldsymbol { b } = \\left[ \\begin{array} { l l l } { b _ { 1 } } & { b _ { 2 } } & { b _ { 3 } } \\end{array} \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设高和宽分别为2个像素的图像样本i的特征为\n",
    "$$x^{(i)} = \\left[ \\begin{array} { l l 1 1} { x _ { 1 }^{(i)} } & { x _ { 2 }^{(i)} } & { x _ { 3 }^{(i)} }&{x_{4}^{(i)}} \\end{array} \\right]$$,\n",
    "输出层的输出为\n",
    "$$\\boldsymbol { o^{(i)} } = \\left[ \\begin{array} { l l l } { o _ { 1 }^{(i)} } & { o _ { 2 }^{(i)}} & { o _ { 3 }^{(i)} } \\end{array} \\right]$$,\n",
    "预测为狗、猫或者鸡的概率分布为\n",
    "$$\\boldsymbol {\\hat{\\boldsymbol{y}}^{(i)} } = \\left[ \\begin{array} { l l l } { \\hat{y} _ { 1 }^{(i)} } & { \\hat{y} _ { 2 }^{(i)}} & { \\hat{y} _ { 3 }^{(i)} } \\end{array} \\right]$$,\n",
    "\n",
    "\n",
    "softmax回归对样本i分类的矢量计算表达式为\n",
    "$$o^{(i)}=x^{(i)} \\boldsymbol{W} +b$$\n",
    "$$\\hat{y}^{(i)} = softmax(o^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 小批量样本分类的矢量计算表达式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了进一步的提升效率，我们通常对小批量的数据做矢量计算。广义上讲，给定一个小批量样本，其批量大小为n，输入个数(特征个数)为d，输出个数为(类别数)为q。\n",
    "设批量特征为$\\boldsymbol{X}∈R^{nxd}$。假设softmax回归的权重和偏差参数为$\\boldsymbol{W}∈R^{d✖️q}和\\boldsymbol{b}∈R^{1✖️q}$ softmax回归的矢量计算表达式为\n",
    "$$\\boldsymbol{O}=\\boldsymbol{X}\\boldsymbol{W}+\\boldsymbol{b}$$\n",
    "$$\\boldsymbol{\\hat{Y}} = softmax(\\boldsymbol{O})$$\n",
    "\n",
    "\n",
    "其中的假发运算使用了广播机制，$\\boldsymbol{O},\\boldsymbol{\\hat{Y}}∈R^{n✖️q}且这两个矩阵的第i行分别为样本i的输出o^{(i)}和概率分布\\hat{y}^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 交叉熵损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用softmax运算后可以方便的与离散标签计算误差。我们已经知道，softmax运算变成一个合理的类别预测分布。实际上，真实标签也可以用类别分布表达：对于样本i，我们可以构造向量$y^{(i)} ∈R^q,使其第y^{(i)}个元素为1，其余为0.这样我们的训练目标可以设置为使预测概率分布\\hat{y}^{(i)}尽可能的接近真实的标签概率分布y^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以像线性回归那样使用平方损失函数$||\\hat{y}^{(i)}-y^{(i)}||^2/2$，然而想要预测出正确的分类结果，并不需要预测概率完全等于标签概率。交叉熵(cross entropy)是一个常用的衡量方法：\n",
    "$$\n",
    "H \\left( \\boldsymbol { y } ^ { ( i ) } , \\hat { \\boldsymbol { y } } ^ { ( i ) } \\right) = - \\sum _ { j = 1 } ^ { q } y _ { j } ^ { ( i ) } \\log \\hat { y } _ { j } ^ { ( i ) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设训练样本的样本数为n,交叉熵损失函数定义为\n",
    "$\\ell\\left({Θ}\\right)$  = $\\frac{1}{n}\\sum_{i=1}^{n}H\\left(\\boldsymbol{y^{(i)}},\\hat{\\boldsymbol{y}}^{(i)}\\right)$\n",
    "\n",
    "\n",
    "$Θ$代表模参数吗，具体细节可以参考\n",
    "[讨论区](https://discuss.gluon.ai/t/topic/6403/7)\n",
    "[博客](https://blog.csdn.net/xg123321123/article/details/80781611)\n",
    "```\n",
    "cross entropy一般是用来量化两个机率分布之间的差距的\n",
    "举个例子，你现在要预测一张图片是狗或猫\n",
    "你的模型得到的概率是\n",
    "狗 = 0.4， 猫 = 0.6\n",
    "而真实的概率则是\n",
    "狗 = 0.0， 猫 = 1.0\n",
    "\n",
    "那么预测出来的概率和真实的概率，两者之间的差距有多大呢？这就是cross entropy要量化的事情了\n",
    "根据上述的例子，我们可知道cross entropy为\n",
    "\n",
    "-( 0.0 * log(0.4) + 1.0*log(0.6) ) = 0.22\n",
    "\n",
    "0.22代表的是你的model预测出来的概率和真实的概率之间，差距有多大\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 模型预测以及评价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练好softmax回归模型后，给定任意样本特征，就可以预测出每个输出类别的概率。通常我们把预测概率最大类别作为输出类别。如果它与真实标签一致，说明预测是正确的。后续分类问题将采用准确率accuracy 来评估模型的表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- softmax回归适用于分类问题，它使用softmax运算输出类别的概率分布\n",
    "- softmax回归是一个单层神经网络，输出个数等于分类问题中的类别个数\n",
    "- 交叉熵适合衡量两个概率分布的差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
